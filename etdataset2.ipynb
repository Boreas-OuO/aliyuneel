{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "O_size=720\n",
    "class DFTDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 window_size=96,\n",
    "                 O_size=720,\n",
    "                 flip=True,\n",
    "                 norm=True):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mean = data.mean()\n",
    "        self.std = data.std()\n",
    "        if norm:\n",
    "            self.data = self.transform(data)\n",
    "        else:\n",
    "            self.data = Data\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.O_size = O_size\n",
    "        self.flip = flip\n",
    "        self.dataset_len = len(self.data) - window_size - O_size + 1\n",
    "        self.slicesX_ = np.zeros((self.dataset_len, window_size))\n",
    "        self.slicesT_ = np.zeros((self.dataset_len, O_size))\n",
    "\n",
    "        # 构建输入和目标序列\n",
    "        for i in range(self.dataset_len):\n",
    "            self.slicesX_[i] = self.data[i:i + window_size]\n",
    "            self.slicesT_[i] = self.data[i + window_size:i + window_size + O_size]\n",
    "        \n",
    "        self.slicesX = self.slicesX_\n",
    "        self.slicesT = self.slicesT_\n",
    "        self.input_DFTtri = self.slices2DFTtri(self.slicesX)\n",
    "        self.slicesT = self.slicesT\n",
    "        #self.input_DFTtri = ei.rearrange(self.slices2DFTtri(self.slicesX),'b h w c -> b c h w')\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_DFTtri = self.input_DFTtri[idx]\n",
    "        target_seq = self.slicesT[idx]\n",
    "        \n",
    "        return input_DFTtri, target_seq\n",
    "    def slices2DFTtri(self, \n",
    "                      slicesX, \n",
    "                      without_f0=False):\n",
    "        timeSteps = slicesX.shape[0]\n",
    "        windowSize = slicesX.shape[1]\n",
    "        max_freq = int((windowSize + 3) / 2)  # int()函数只保留整数部分\n",
    "        # complex128就是64+64的复数\n",
    "        # timestep是矩阵的数量，后面两个参数决定了每个矩阵的维度，生成1247个12*12的matrix\n",
    "        DFTtri = np.zeros([timeSteps, windowSize, windowSize], dtype = np.complex64) #降低精度防止爆内存\n",
    "        for i in tqdm(range(timeSteps),desc=\"Loading Data\",ncols=100):\n",
    "            for j in range(windowSize):\n",
    "                fft = np.fft.fft(slicesX[i, -(1+j):])\n",
    "                DFTtri[i, :(j+1), j] = fft[::-1]\n",
    "                if self.flip:\n",
    "                    DFTtri[i, j, :(j+1)] = fft[::-1] # Flip padding\n",
    "        if without_f0:\n",
    "            DFTtri = DFTtri[:,1:,1:]\n",
    "        DFTtriDescartes = np.stack([DFTtri.real, DFTtri.imag], axis=-3)\n",
    "        return DFTtriDescartes\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n",
    "\n",
    "class MAN(nn.Module):\n",
    "    def __init__(self, num_classes=O_size):\n",
    "        super(MAN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 256, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "def main():\n",
    "    # current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    # os.chdir(current_dir)\n",
    "\n",
    "    etdata = pd.read_csv('./ETTm2.csv')\n",
    "    ot = etdata['OT'].values\n",
    "\n",
    "\n",
    "    dft_dataset = DFTDataset(ot)\n",
    "\n",
    "    dataset_size = len(dft_dataset)\n",
    "    train_size = int(0.7 * dataset_size)\n",
    "    val_size = int(0.1 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "    dataloader = torch.utils.data.DataLoader(dft_dataset, \n",
    "                                             batch_size=256, \n",
    "                                             shuffle=False,\n",
    "                                             pin_memory=True)\n",
    "\n",
    "    model = MAN()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 50\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    try:\n",
    "        checkpoint = torch.load('./save_models/model_840.pth')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    except:\n",
    "        pass\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, batch_loss, vali_loss, test_loss = 0,0,0,0\n",
    "        count=0\n",
    "        for inputs, targets in tqdm(dataloader, \n",
    "                                    desc=f'Epoch {epoch + 1}/{num_epochs}',\n",
    "                                    ncols=100, \n",
    "                                    leave=False):\n",
    "            inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "            if count< train_size:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                invtrans_outputs = dft_dataset.inverse_transform(outputs)\n",
    "                invtrans_targets = dft_dataset.inverse_transform(targets)\n",
    "                batch_loss = criterion(invtrans_outputs, invtrans_targets)\n",
    "                norm_train_loss = loss.item()\n",
    "                running_loss += batch_loss.item() * inputs.size(0)\n",
    "            elif count>= train_size and count< train_size+val_size:\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "                norm_vali_loss = criterion(outputs,targets).item()\n",
    "                invtrans_outputs = dft_dataset.inverse_transform(outputs)\n",
    "                invtrans_targets = dft_dataset.inverse_transform(targets)\n",
    "                loss = criterion(invtrans_outputs, invtrans_targets)\n",
    "                vali_loss += loss.item() * inputs.size(0)\n",
    "            elif count>= train_size+val_size:\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "                norm_test_loss = criterion(outputs,targets).item()\n",
    "                invtrans_outputs = dft_dataset.inverse_transform(outputs)\n",
    "                invtrans_targets = dft_dataset.inverse_transform(targets)\n",
    "                loss = criterion(invtrans_outputs, invtrans_targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            count+=inputs.size(0)\n",
    "        epoch_loss = running_loss / train_size\n",
    "        vali_loss = vali_loss / val_size\n",
    "        test_loss = test_loss / test_size\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train_Loss: {epoch_loss:.4f}/{norm_train_loss:.4f}, \\\n",
    "              Vali_Loss:{vali_loss}/{norm_vali_loss:.4f}, Test_Loss:{test_loss}/{norm_test_loss:.4f}\")\n",
    "        if epoch %10 ==0:\n",
    "            checkpoint = {'epoch':epoch,\n",
    "            \n",
    "                          'model':model.state_dict(),\n",
    "                          'optimizer':optimizer.state_dict()}\n",
    "            torch.save(checkpoint,f\"./save_models/model_{epoch}.pth\")\n",
    "    print(\"Finished Training\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|█████████████████████████████████████████| 68865/68865 [00:25<00:00, 2731.61it/s]\n",
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train_Loss: 86.8776/0.1912,               Vali_Loss:42.52774871285489/0.7794, Test_Loss:200.6493088262632/3.0306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/workspace/Dftcnn/etdataset2.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://dsw-gateway-cn-shenzhen.data.aliyun.com/mnt/workspace/Dftcnn/etdataset2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m     writer\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    <a href='vscode-notebook-cell://dsw-gateway-cn-shenzhen.data.aliyun.com/mnt/workspace/Dftcnn/etdataset2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://dsw-gateway-cn-shenzhen.data.aliyun.com/mnt/workspace/Dftcnn/etdataset2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m     main()\n",
      "\u001b[1;32m/mnt/workspace/Dftcnn/etdataset2.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shenzhen.data.aliyun.com/mnt/workspace/Dftcnn/etdataset2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     invtrans_targets \u001b[39m=\u001b[39m dft_dataset\u001b[39m.\u001b[39minverse_transform(targets)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shenzhen.data.aliyun.com/mnt/workspace/Dftcnn/etdataset2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     batch_loss \u001b[39m=\u001b[39m criterion(invtrans_outputs, invtrans_targets)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dsw-gateway-cn-shenzhen.data.aliyun.com/mnt/workspace/Dftcnn/etdataset2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     norm_train_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shenzhen.data.aliyun.com/mnt/workspace/Dftcnn/etdataset2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shenzhen.data.aliyun.com/mnt/workspace/Dftcnn/etdataset2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39melif\u001b[39;00m count\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m train_size \u001b[39mand\u001b[39;00m count\u001b[39m<\u001b[39m train_size\u001b[39m+\u001b[39mval_size:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    etdata = pd.read_csv('./ETTm2.csv')\n",
    "    ot = etdata['OT'].values\n",
    "\n",
    "    dft_dataset = DFTDataset(ot)\n",
    "\n",
    "    dataset_size = len(dft_dataset)\n",
    "    train_size = int(0.7 * dataset_size)\n",
    "    val_size = int(0.1 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "    dataloader = DataLoader(dft_dataset, \n",
    "                             batch_size=256, \n",
    "                             shuffle=False,\n",
    "                             pin_memory=True)\n",
    "\n",
    "    model = MAN()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 50\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    try:\n",
    "        checkpoint = torch.load('./save_models/model_840.pth')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter('logs')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, batch_loss, vali_loss, test_loss = 0,0,0,0\n",
    "        count=0\n",
    "        for inputs, targets in tqdm(dataloader, \n",
    "                                    desc=f'Epoch {epoch + 1}/{num_epochs}',\n",
    "                                    ncols=100, \n",
    "                                    leave=False):\n",
    "            inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "            if count< train_size:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                invtrans_outputs = dft_dataset.inverse_transform(outputs)\n",
    "                invtrans_targets = dft_dataset.inverse_transform(targets)\n",
    "                batch_loss = criterion(invtrans_outputs, invtrans_targets)\n",
    "                norm_train_loss = loss.item()\n",
    "                running_loss += batch_loss.item() * inputs.size(0)\n",
    "            elif count>= train_size and count< train_size+val_size:\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "                norm_vali_loss = criterion(outputs,targets).item()\n",
    "                invtrans_outputs = dft_dataset.inverse_transform(outputs)\n",
    "                invtrans_targets = dft_dataset.inverse_transform(targets)\n",
    "                loss = criterion(invtrans_outputs, invtrans_targets)\n",
    "                vali_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # Plot the predicted and real values of the last item of the last batch of the validation set\n",
    "                if count + inputs.size(0) == val_size:\n",
    "                    last_output = invtrans_outputs[-1].detach().cpu().numpy()\n",
    "                    last_target = invtrans_targets[-1].detach().cpu().numpy()\n",
    "                    print(last_output)\n",
    "                    fig, ax = plt.subplots()\n",
    "                    ax.plot(last_output, label='Predicted')\n",
    "                    ax.plot(last_target, label='Real')\n",
    "                    ax.legend()\n",
    "                    writer.add_figure('Validation/Predicted_vs_Real', fig, global_step=epoch)\n",
    "                    \n",
    "            elif count>= train_size+val_size:\n",
    "                model.eval()\n",
    "                outputs = model(inputs)\n",
    "                norm_test_loss = criterion(outputs,targets).item()\n",
    "                invtrans_outputs = dft_dataset.inverse_transform(outputs)\n",
    "                invtrans_targets = dft_dataset.inverse_transform(targets)\n",
    "                loss = criterion(invtrans_outputs, invtrans_targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            count+=inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / train_size\n",
    "        vali_loss = vali_loss / val_size\n",
    "        test_loss = test_loss / test_size\n",
    "        \n",
    "        # Log losses to TensorBoard\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Loss/validation', vali_loss, epoch)\n",
    "        writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train_Loss: {epoch_loss:.4f}/{norm_train_loss:.4f}, \\\n",
    "              Vali_Loss:{vali_loss}/{norm_vali_loss:.4f}, Test_Loss:{test_loss}/{norm_test_loss:.4f}\")\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "            torch.save(checkpoint, f\"./save_models/model_{epoch}.pth\")\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "    writer.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'last_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/workspace/Dftcnn/etdataset2.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dsw-gateway-cn-shenzhen.data.aliyun.com/mnt/workspace/Dftcnn/etdataset2.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m last_output\n",
      "\u001b[0;31mNameError\u001b[0m: name 'last_output' is not defined"
     ]
    }
   ],
   "source": [
    "last_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFTDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 window_size=96,\n",
    "                 O_size=720,\n",
    "                 flip=True,\n",
    "                 norm=True):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mean = data.mean()\n",
    "        self.std = data.std()\n",
    "        if norm:\n",
    "            self.data = self.transform(data)\n",
    "        else:\n",
    "            self.data = Data\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.O_size = O_size\n",
    "        self.flip = flip\n",
    "        self.dataset_len = len(self.data) - window_size - O_size + 1\n",
    "        self.slicesX_ = np.zeros((self.dataset_len, window_size))\n",
    "        self.slicesT_ = np.zeros((self.dataset_len, O_size))\n",
    "\n",
    "        # 构建输入和目标序列\n",
    "        for i in range(self.dataset_len):\n",
    "            self.slicesX_[i] = self.data[i:i + window_size]\n",
    "            self.slicesT_[i] = self.data[i + window_size:i + window_size + O_size]\n",
    "        \n",
    "        self.slicesX = self.slicesX_\n",
    "        self.slicesT = self.slicesT_\n",
    "        self.input_DFTtri = self.slices2DFTtri(self.slicesX)\n",
    "        self.slicesT = self.slicesT\n",
    "        #self.input_DFTtri = ei.rearrange(self.slices2DFTtri(self.slicesX),'b h w c -> b c h w')\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_DFTtri = self.input_DFTtri[idx]\n",
    "        target_seq = self.slicesT[idx]\n",
    "        \n",
    "        return input_DFTtri, target_seq\n",
    "    def slices2DFTtri(self, \n",
    "                      slicesX, \n",
    "                      without_f0=False):\n",
    "        timeSteps = slicesX.shape[0]\n",
    "        windowSize = slicesX.shape[1]\n",
    "        max_freq = int((windowSize + 3) / 2)  # int()函数只保留整数部分\n",
    "        # complex128就是64+64的复数\n",
    "        # timestep是矩阵的数量，后面两个参数决定了每个矩阵的维度，生成1247个12*12的matrix\n",
    "        DFTtri = np.zeros([timeSteps, windowSize, windowSize], dtype = np.complex64) #降低精度防止爆内存\n",
    "        for i in tqdm(range(timeSteps),desc=\"Loading Data\",ncols=100):\n",
    "            for j in range(windowSize):\n",
    "                fft = np.fft.fft(slicesX[i, -(1+j):])\n",
    "                DFTtri[i, :(j+1), j] = fft[::-1]\n",
    "                if self.flip:\n",
    "                    DFTtri[i, j, :(j+1)] = fft[::-1] # Flip padding\n",
    "        if without_f0:\n",
    "            DFTtri = DFTtri[:,1:,1:]\n",
    "        DFTtriDescartes = np.stack([DFTtri.real, DFTtri.imag], axis=-3)\n",
    "        return DFTtriDescartes\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n",
    "class MAN(nn.Module):\n",
    "    def __init__(self, num_classes=96):\n",
    "        super(MAN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 256, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFTTRI:\n",
    "    def __init__(self, pred_len, num_epochs=50, lr=1e-3): \n",
    "        self.pred_len = pred_len                                                                            \n",
    "        self.model = MAN(num_classes=pred_len)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)     \n",
    "        self.num_epochs = num_epochs\n",
    "        self.writer = SummaryWriter('logs')\n",
    "        self.epoch = 0\n",
    "        self.loss = 0\n",
    "\n",
    "    def load_data(self, ot, name='edt', batch_size=256):\n",
    "        self.name = name + str(self.pred_len)\n",
    "        dataset_size = ot.shape[0]\n",
    "        self.train_size = int(0.7 * dataset_size)\n",
    "        self.val_size = int(0.1 * dataset_size)\n",
    "        self.test_size = dataset_size - self.train_size - self.val_size\n",
    "        \n",
    "\n",
    "        self.trainset = DFTDataset(ot[:self.train_size],O_size=self.pred_len)\n",
    "        self.valiset = DFTDataset(ot[self.train_size:self.train_size+self.val_size],O_size=self.pred_len)\n",
    "        self.testset = DFTDataset(ot[self.train_size+self.val_size:],O_size=self.pred_len)\n",
    "        self.trainloader = torch.utils.data.DataLoader(self.trainset, \n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=True)\n",
    "        self.valiloader = torch.utils.data.DataLoader(self.valiset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=True)\n",
    "        self.testloader = torch.utils.data.DataLoader(self.testset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=True)   \n",
    "    def mse(self, outputs, targets):\n",
    "        return ((outputs - targets) ** 2).mean().item()\n",
    "\n",
    "    def mae(self, outputs, targets):\n",
    "        return (outputs - targets).abs().mean().item()\n",
    "    \n",
    "    def metric(self, outputs, targets):\n",
    "        return self.mse(outputs, targets), self.mae(outputs, targets)\n",
    "\n",
    "    def _plot(self, outputs, targets, mode):\n",
    "        final_output = outputs[-1]\n",
    "        final_target = targets[-1]\n",
    "        # 绘制final_output和final_target，并保存到writer的logs中\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(final_output.detach().cpu().numpy(), label='Final Output')\n",
    "        ax.plot(final_target.detach().cpu().numpy(), label='Final Target')\n",
    "        ax.legend()\n",
    "        self.writer.add_figure(f'{self.name}/{mode}/Final_Output_vs_Target', fig, global_step=self.epoch)\n",
    "    \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        mse_loss, mae_loss, inv_mse_loss, inv_mae_loss = 0, 0, 0, 0\n",
    "        for inputs, targets in self.trainloader:\n",
    "            inputs, targets = inputs.float().to(self.device), targets.float().to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            invtrans_outputs = self.trainset.inverse_transform(outputs)\n",
    "            invtrans_targets = self.trainset.inverse_transform(targets)\n",
    "            ori_loss = self.metric(outputs, targets)\n",
    "            inv_loss = self.metric(invtrans_outputs, invtrans_targets)\n",
    "            mse_loss += ori_loss[0]\n",
    "            mae_loss += ori_loss[1]\n",
    "            inv_mse_loss += inv_loss[0]\n",
    "            inv_mae_loss += inv_loss[1]\n",
    "        \n",
    "        mse_loss /= len(self.trainloader)\n",
    "        mae_loss /= len(self.trainloader)\n",
    "        inv_mse_loss /= len(self.trainloader)\n",
    "        inv_mae_loss /= len(self.trainloader)\n",
    "        \n",
    "        self.loss = mse_loss \n",
    "        \n",
    "        self.writer.add_scalar(f'{self.name}/Train_Loss/mse', mse_loss, self.epoch)\n",
    "        self.writer.add_scalar(f'{self.name}/Train_Loss/mae', mae_loss, self.epoch)\n",
    "        self.writer.add_scalar(f'{self.name}/Train_Loss/mse_inv', inv_mse_loss, self.epoch)\n",
    "        self.writer.add_scalar(f'{self.name}/Train_Loss/mae_inv', inv_mae_loss, self.epoch)\n",
    "        self._plot(outputs, targets, 'train')\n",
    "    \n",
    "    def vali(self):\n",
    "        self.model.eval()\n",
    "        mse_loss, mae_loss, inv_mse_loss, inv_mae_loss = 0, 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.valiloader:\n",
    "                inputs, targets = inputs.float().to(self.device), targets.float().to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                invtrans_outputs = self.valiset.inverse_transform(outputs)\n",
    "                invtrans_targets = self.valiset.inverse_transform(targets)\n",
    "                ori_loss = self.metric(outputs, targets)\n",
    "                inv_loss = self.metric(invtrans_outputs, invtrans_targets)\n",
    "                mse_loss += ori_loss[0]\n",
    "                mae_loss += ori_loss[1]\n",
    "                inv_mse_loss += inv_loss[0]\n",
    "                inv_mae_loss += inv_loss[1]\n",
    "\n",
    "        mse_loss /= len(self.valiloader)\n",
    "        mae_loss /= len(self.valiloader)\n",
    "        inv_mse_loss /= len(self.valiloader)\n",
    "        inv_mae_loss /= len(self.valiloader)\n",
    "        self.writer.add_scalar(f'{self.name}/Vali_Loss/mse', mse_loss, self.epoch)\n",
    "        self.writer.add_scalar(f'{self.name}/Vali_Loss/mae', mae_loss, self.epoch)\n",
    "        self.writer.add_scalar(f'{self.name}/Vali_Loss/mse_inv', inv_mse_loss, self.epoch)\n",
    "        self.writer.add_scalar(f'{self.name}/vali_Loss/mae_inv', inv_mae_loss, self.epoch)\n",
    "        self._plot(outputs, targets, 'vali')\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        mse_loss, mae_loss, inv_mse_loss, inv_mae_loss = 0, 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.testloader:\n",
    "                inputs, targets = inputs.float().to(self.device), targets.float().to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                invtrans_outputs = self.testset.inverse_transform(outputs)\n",
    "                invtrans_targets = self.testset.inverse_transform(targets)\n",
    "                ori_loss = self.metric(outputs, targets)\n",
    "                inv_loss = self.metric(invtrans_outputs, invtrans_targets)\n",
    "                mse_loss += ori_loss[0]\n",
    "                mae_loss += ori_loss[1]\n",
    "                inv_mse_loss += inv_loss[0]\n",
    "                inv_mae_loss += inv_loss[1]\n",
    "\n",
    "        mse_loss /= len(self.testloader)\n",
    "        mae_loss /= len(self.testloader)\n",
    "        inv_mse_loss /= len(self.testloader)\n",
    "        inv_mae_loss /= len(self.testloader)\n",
    "        self.writer.add_scalar(f'{self.name}/Test_Loss/mse', mse_loss, self.epoch)\n",
    "        self.writer.add_scalar(f'{self.name}/Test_Loss/mae', mae_loss, self.epoch)\n",
    "        self.writer.add_scalar(f'{self.name}/Test_Loss/mse_inv', inv_mse_loss, self.epoch)\n",
    "        self.writer.add_scalar(f'{self.name}/Test_Loss/mae_inv', inv_mae_loss, self.epoch)\n",
    "        self._plot(outputs, targets, 'test')\n",
    "\n",
    "    def run(self):\n",
    "        with tqdm(total=self.num_epochs, desc='Training', ncols=100, leave=False) as pbar:\n",
    "            for self.epoch in range(self.num_epochs):\n",
    "                self.train()\n",
    "                self.vali()\n",
    "                self.test()\n",
    "                pbar.set_description(f'Training (Loss: {self.loss:.4f})')\n",
    "                pbar.update(1)\n",
    "\n",
    "                if self.epoch % 50 == 0 and self.epoch:\n",
    "                    # 保存模型\n",
    "                    torch.save(self.model.state_dict(), f\"./save_models/model_{self.epoch}.pth\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "etdata = pd.read_csv('./ETTm2.csv')\n",
    "ot = etdata['OT'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfttri = DFTTRI(num_epochs=1000 , pred_len=720, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|█████████████████████████████████████████| 47961/47961 [00:17<00:00, 2704.96it/s]\n",
      "Loading Data: 100%|███████████████████████████████████████████| 6153/6153 [00:02<00:00, 2726.95it/s]\n",
      "Loading Data: 100%|█████████████████████████████████████████| 13121/13121 [00:04<00:00, 2707.09it/s]\n"
     ]
    }
   ],
   "source": [
    "dfttri.load_data(ot, name='edt', batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (Loss: 1.0072):   1%|▎                                  | 8/1000 [01:54<3:56:52, 14.33s/it]"
     ]
    }
   ],
   "source": [
    "dfttri.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
